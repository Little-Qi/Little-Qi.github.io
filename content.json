{"pages":[{"title":"About","text":"Rongbo就是我的名字，丝毫没有什么特殊的含义（每次想网络昵称、网站Title等等时都极为头大） 当然不仅是起名字，自从高考之后语文能力退化，根本不知道这里要写点什么好吗/(ㄒoㄒ)/~~ 不过问题不大…… 本人目前还在大学中享受生活，但也深深卷在计科的火海中，深知只有学习……🤦‍ 最近想写博客了，不得不说如果卸载掉抖音就会有很多空余时间读书学习。之前自己用Vue纯手工打造了一个前后端日志系统，但在网站上随便抒发抒发记录记录还行，但写一篇博客总归不是很好用。也发懒不想再自己手码一个了，就借用Hexo+Icarus在github上搭了一个，回头看看有机会放到服务器上事实上我第二天就放上去了，就还快一点，毕竟github国内访问emmm大家都懂。 特此感谢Github Hexo Icarus 腾讯云Cos对象存储 和很多图片来源👀 博客主要写一些学习笔记，学习成果记录，一些困难的作业，一些踩过的坑等等 相关问题可在评论中留言，也可以发送邮箱到583116128@qq.com 本站文章如无特别声明，均为原创，采用 知识共享署名 4.0 国际许可协议 进行许可，转载请注明来源。 未完待续……","link":"/about/index.html"}],"posts":[{"title":"《深度学习》（花书） 1.1 引言","text":"人工智能 Artificial Intelligence 深度学习 层次化的概念 让计算机从经验获取知识 （通过构建简单的概念学习复杂的概念） 图 或许层次很多 DeepLearning 形象化任务 与 非形象化（直观的）任务 对计算机来说的难易程度 硬编码 知识库 （konwledge base） eg : Cyc 机器学习 machine learning 该方法可以解决涉及现实世界的问题并做出决策 例如 逻辑回归的简单算法可以决定是否建议进行刨妇产，朴素贝叶斯算法可以区分合法邮件与垃圾邮件 这些机器学习算法依赖于给定数据的表示 例如判断是否需要刨妇产 其算法并不会直接进行孕妇的体检 而是医生将特定的数据报告提供给算法 假如只将核磁共振的片子作为特征输入 将不具发现任何相关性的能力 数据表示 的重要性 如笛卡尔坐标与极坐标对于某些问题的处理的简易程度 当我们已知一个特征与某问题的相关性时，我们可以通过构建特征集提供给简单的机器学习算法，然而对于更多的任务来说，我们很难知道应该提取哪些特征。例如我们知道通过车轮辨别车，但又并不知道该如何辨别车轮。这种情况有时可以通过表示学习（representation learning 指使用机器学习来发掘表示自身）进行解决，典型例子是自编码器，其包括编码器与解码器，二者对原始数据进行形式的转换，目标是经过编码与解码后保留更多的信息。 变差因素 factors of variation 当设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能解释观察数据的变差因素(factors of variation)。在此背景下,“因素”这个词仅指代影响的不同来源：因素通常不是乘性组合。这些因素通常是不能被直接观察到的量。相反：它们可能是现实世界中观察不到的物体或者不可观测的力，但会影响可观测的量。为了对观察到的数据提供有用的简化解释或推断其原因.它们还可能以概念的形式存在于人类的思维中。它们可以被看作数据的概念或者抽象，帮助我们了解这些数据的丰富多样性。当分析语音记录时，变差因素包括说话者的年龄、性别，他们的口音和他们正在说的词语。当分析汽车的图像时，变差因素包括汽车的位置、它的颜色、太阳的角度和亮度。 ​ 可以看到 从原始数据提炼出这些抽象的特征十分困难 在这类问题中 表示学习并不能有效地帮到我们 深度学习可以通过简单的表示来表达复杂的表示，解决上述问题。 深度学习模型的典型例子是前馈深度网络或多层感知机（multilayer perceptron, MLP）。多层感知机仅仅是一个将一组输入值映射到输出值的数学函数。该函数由许多较简单的函数复合而成。我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。 除了用”表示“的角度解释深度学习，其亦可以理解为 ”深度促使计算机学习一个多步骤的计算机程序“ 度量模型深度的方式： 基于评估架构所需执行的顺序指令的数目 描述 概念彼此如何关联 的图 的深度（深度概率模型） 相比于传统机器学习，深度学习这种特定类型的机器学习 研究的模型设计到更多的 学到功能 和 学到概念 的组合，更加灵活与强大。 &nbsp;&nbsp; Deep Learning,https://www.deeplearningbook.org","link":"/2022/03/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01.1%E5%BC%95%E8%A8%80/"},{"title":"Daily Feelings-真相","text":"“其实我说什么并不重要，因为我发现大家都是一样，只想看到自己想看到。你觉得他是怎么样子的，你就会自己去找各种捕风捉影，各种小的东西，去想要验证你自己的观点。别人跟你说，你就会觉得，不是这样子。但是谁在乎呢？” ——Tian 无论是微博还是知乎，又或是其他各种地方，每个人都操着自己的观点长篇大论，甚至不惜和不相识者吵得不可开交。我们都晓得上面的观点，但总是忽略真相，哪怕是在巨大的证据面前寻找其他边边角角，仿佛在自己的心面前，别的已经不重要了……","link":"/2022/03/05/DailyFeelings/"},{"title":"《深度学习》（花书） 2 线性代数","text":"第二章 线性代数2.1 标量 向量 矩阵 张量 标量 scalar 向量 vector 如果每个元素都属于R，并且该向量有n 个元素，那么该向量属于实数集R的n 次笛卡尔乘积构成的集合，记为Rn。 矩阵 matrix A ∈ Rm×n Ai,j 张量 tensor A Ai,j,k 矩阵的转置 transpose 以 主对角线 main diagonal 为轴的镜像 广播 broadcasting 2.2 矩阵和向量相乘 矩阵的乘积不满足交换律，但两个向量的点积满足交换律 $ x^Ty = y^Tx $ 矩阵乘积的转置 $（AB）^T = B^TA^T$ 线性方程组 $Ax = b$ 2.3 单位矩阵和逆矩阵 单位矩阵 identity matrix $ I_n $. $ I_n x = x$ 逆矩阵 matrix inversion $ A^{-1} $ $ A^{-1} A = I_n $ $$\\displaylines{ \\begin{array}{c} A x=b \\\\ A^{-1} A x=A^{-1} b \\\\ I_{n} x=A^{-1} b \\\\ x=A^{-1} b \\end{array} }$$ 当然，这取决于我们能否找到一个逆矩阵$A^{-1}$。当逆矩阵$A^{-1}$存在时，有几种不同的算法都能找到它的闭解形式。理论上，相同的逆矩阵可用于多次求解不同向量b的方程。然而，逆矩阵$A^{-1}$主要是作为理论工具使用的，并不会在大多数软件应用程序中实际使用。这是因为逆矩阵$A^{-1}$在数字计算机上只能表现出有限的精度，有效使用向量 b 的算法通常可以得到更精确的$x$。 2.4 线性相关和生成子空间 线性组合 linear combination 向量乘以标量的和 生成子空间 span 确定$Ax = b$ 是否有解相当于确定向量$b$是否在$A$列向量的生成子空间中。这个特殊的生成子空间被称为$A$的 列空间 （column space）或者A的 值域 （range）。 线性相关 linear dependence 线性无关 linearly independent 可逆矩阵 定理 以下命题同真或同假 （下图引自《线性代数及其应用》 这里的重点是 方阵 + 线性无关） 对于方阵而言，左逆和右逆相等 2.5 范数范数 在学校线性代数的讲解中并没有提到，但在机器学习中应用广泛，衡量向量和矩阵大小等，具体如下。 2.6 特殊的矩阵和向量 对角矩阵 diagonal matrix 对称 （symmetric） 矩阵 单位向量 是具有单位范数的向量 正交 orthogonal 标准正交 正交矩阵（$A^{-1} = A^T$) 2.7 特征分解 $Av = \\lambda v $ 其中$v$ 是特征向量 eigenvector $\\lambda$ 为特征值 $ sv $ 同样是特征向量 $s$ 是非零实数 $A = V \\operatorname{diag}(\\lambda) V^{-1}$ $A=Q \\Lambda Q^{\\top}$ 未完待续……","link":"/2022/03/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"}],"tags":[{"name":"引言","slug":"引言","link":"/tags/%E5%BC%95%E8%A8%80/"},{"name":"《深度学习》","slug":"《深度学习》","link":"/tags/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B/"},{"name":"感想","slug":"感想","link":"/tags/%E6%84%9F%E6%83%B3/"},{"name":"记录","slug":"记录","link":"/tags/%E8%AE%B0%E5%BD%95/"},{"name":"线性代数","slug":"线性代数","link":"/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"}],"categories":[{"name":"日常","slug":"日常","link":"/categories/%E6%97%A5%E5%B8%B8/"},{"name":"读书笔记","slug":"读书笔记","link":"/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"《深度学习》","slug":"读书笔记/《深度学习》","link":"/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B/"},{"name":"感想","slug":"日常/感想","link":"/categories/%E6%97%A5%E5%B8%B8/%E6%84%9F%E6%83%B3/"}]}