{"pages":[{"title":"About","text":"Rongbo就是我的名字，丝毫没有什么特殊的含义（每次想网络昵称、网站Title等等时都极为头大） 当然不仅是起名字，自从高考之后语文能力退化，根本不知道这里要写点什么好吗/(ㄒoㄒ)/~~ 不过问题不大…… 本人目前还在大学中享受生活，但也深深卷在计科的火海中，深知只有学习……🤦‍ 最近想写博客了，不得不说如果卸载掉抖音就会有很多空余时间读书学习。之前自己用Vue纯手工打造了一个前后端日志系统，但在网站上随便抒发抒发记录记录还行，但写一篇博客总归不是很好用。也发懒不想再自己手码一个了，就借用Hexo+Icarus在github上搭了一个，回头看看有机会放到服务器上事实上我第二天就放上去了，就还快一点，毕竟github国内访问emmm大家都懂。 特此感谢Github Hexo Icarus 腾讯云Cos对象存储 和很多图片来源👀 博客主要写一些学习笔记，学习成果记录，一些困难的作业，一些踩过的坑等等 相关问题可在评论中留言，也可以发送邮箱到583116128@qq.com 本站文章如无特别声明，均为原创，采用 知识共享署名 4.0 国际许可协议 进行许可，转载请注明来源。 未完待续……","link":"/about/index.html"}],"posts":[{"title":"Daily Feelings-真相","text":"“其实我说什么并不重要，因为我发现大家都是一样，只想看到自己想看到。你觉得他是怎么样子的，你就会自己去找各种捕风捉影，各种小的东西，去想要验证你自己的观点。别人跟你说，你就会觉得，不是这样子。但是谁在乎呢？” ——Tian 无论是微博还是知乎，又或是其他各种地方，每个人都操着自己的观点长篇大论，甚至不惜和不相识者吵得不可开交。我们都晓得上面的观点，但总是忽略真相，哪怕是在巨大的证据面前寻找其他边边角角……","link":"/2022/03/05/DailyFeelings/"},{"title":"《深度学习》（花书） 2 线性代数","text":"第二章 线性代数2.1 标量 向量 矩阵 张量 标量 scalar 向量 vector 如果每个元素都属于R，并且该向量有n 个元素，那么该向量属于实数集R的n 次笛卡尔乘积构成的集合，记为Rn。 矩阵 matrix A ∈ Rm×n Ai,j 张量 tensor A Ai,j,k 矩阵的转置 transpose 以 主对角线 main diagonal 为轴的镜像 广播 broadcasting 2.2 矩阵和向量相乘 矩阵的乘积不满足交换律，但两个向量的点积满足交换律 $ x^Ty = y^Tx $ 矩阵乘积的转置 $（AB）^T = B^TA^T$ 线性方程组 $Ax = b$ 2.3 单位矩阵和逆矩阵 单位矩阵 identity matrix $ I_n $. $ I_n x = x$ 逆矩阵 matrix inversion $ A^{-1} $ $ A^{-1} A = I_n $ $$\\begin{eqnarray}A x &amp; = &amp; b \\A^{-1} A x &amp; = &amp; A^{-1} b \\I_{n} x &amp; = &amp; A^{-1} b \\x &amp; = &amp; A^{-1} b\\end{eqnarray}$$ 当然，这取决于我们能否找到一个逆矩阵$A^{-1}$。当逆矩阵$A^{-1}$存在时，有几种不同的算法都能找到它的闭解形式。理论上，相同的逆矩阵可用于多次求解不同向量b的方程。然而，逆矩阵$A^{-1}$主要是作为理论工具使用的，并不会在大多数软件应用程序中实际使用。这是因为逆矩阵$A^{-1}$在数字计算机上只能表现出有限的精度，有效使用向量 b 的算法通常可以得到更精确的$x$。 2.4 线性相关和生成子空间 线性组合 linear combination 向量乘以标量的和 生成子空间 span 确定$Ax = b$ 是否有解相当于确定向量$b$是否在$A$列向量的生成子空间中。这个特殊的生成子空间被称为$A$的 列空间 （column space）或者A的 值域 （range）。 线性相关 linear dependence 线性无关 linearly independent 可逆矩阵 定理 以下命题同真或同假 （下图引自《线性代数及其应用》 这里的重点是 方阵 + 线性无关） 对于方阵而言，左逆和右逆相等 2.5 范数范数 在学校线性代数的讲解中并没有提到，但在机器学习中应用广泛，衡量向量和矩阵大小等，具体如下。 2.6 特殊的矩阵和向量 对角矩阵 diagonal matrix 对称 （symmetric） 矩阵 单位向量 是具有单位范数的向量 正交 orthogonal 标准正交 正交矩阵（$A^{-1} = A^T$) 2.7 特征分解 $Av = \\lambda v $ 其中$v$ 是特征向量 eigenvector $\\lambda$ 为特征值 $ sv $ 同样是特征向量 $s$ 是非零实数 $A = V \\operatorname{diag}(\\lambda) V^{-1}$ $A=Q \\Lambda Q^{\\top}$ 未完待续……","link":"/2022/03/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"title":"《深度学习》（花书） 1.1 引言","text":"人工智能 Artificial Intelligence 深度学习 层次化的概念 让计算机从经验获取知识 （通过构建简单的概念学习复杂的概念） 图 或许层次很多 DeepLearning 形象化任务 与 非形象化（直观的）任务 对计算机来说的难易程度 硬编码 知识库 （konwledge base） eg : Cyc 机器学习 machine learning 该方法可以解决涉及现实世界的问题并做出决策 例如 逻辑回归的简单算法可以决定是否建议进行刨妇产，朴素贝叶斯算法可以区分合法邮件与垃圾邮件 这些机器学习算法依赖于给定数据的表示 例如判断是否需要刨妇产 其算法并不会直接进行孕妇的体检 而是医生将特定的数据报告提供给算法 假如只将核磁共振的片子作为特征输入 将不具发现任何相关性的能力 数据表示 的重要性 如笛卡尔坐标与极坐标对于某些问题的处理的简易程度 当我们已知一个特征与某问题的相关性时，我们可以通过构建特征集提供给简单的机器学习算法，然而对于更多的任务来说，我们很难知道应该提取哪些特征。例如我们知道通过车轮辨别车，但又并不知道该如何辨别车轮。这种情况有时可以通过表示学习（representation learning 指使用机器学习来发掘表示自身）进行解决，典型例子是自编码器，其包括编码器与解码器，二者对原始数据进行形式的转换，目标是经过编码与解码后保留更多的信息。 变差因素 factors of variation 当设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能解释观察数据的变差因素(factors of variation)。在此背景下,“因素”这个词仅指代影响的不同来源：因素通常不是乘性组合。这些因素通常是不能被直接观察到的量。相反：它们可能是现实世界中观察不到的物体或者不可观测的力，但会影响可观测的量。为了对观察到的数据提供有用的简化解释或推断其原因.它们还可能以概念的形式存在于人类的思维中。它们可以被看作数据的概念或者抽象，帮助我们了解这些数据的丰富多样性。当分析语音记录时，变差因素包括说话者的年龄、性别，他们的口音和他们正在说的词语。当分析汽车的图像时，变差因素包括汽车的位置、它的颜色、太阳的角度和亮度。 ​ 可以看到 从原始数据提炼出这些抽象的特征十分困难 在这类问题中 表示学习并不能有效地帮到我们 深度学习可以通过简单的表示来表达复杂的表示，解决上述问题。 深度学习模型的典型例子是前馈深度网络或多层感知机（multilayer perceptron, MLP）。多层感知机仅仅是一个将一组输入值映射到输出值的数学函数。该函数由许多较简单的函数复合而成。我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。 除了用”表示“的角度解释深度学习，其亦可以理解为 ”深度促使计算机学习一个多步骤的计算机程序“ 度量模型深度的方式： 基于评估架构所需执行的顺序指令的数目 描述 概念彼此如何关联 的图 的深度（深度概率模型） 相比于传统机器学习，深度学习这种特定类型的机器学习 研究的模型设计到更多的 学到功能 和 学到概念 的组合，更加灵活与强大。 &nbsp;&nbsp; Deep Learning,https://www.deeplearningbook.org","link":"/2022/03/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01.1%E5%BC%95%E8%A8%80/"},{"title":"《神经网络与深度学习》前馈神经网络","text":"前馈神经网络 ANN 人工神经网络 感知机 神经元特点 神经元是一个多输入、单输出的元件。 神经元是一个具有非线性输入 / 输出特性的元件。表现在只有当来自各神经突触的活动电位达到一定强度后，该神经元才能被激活，释放出神经传递化学物质，发出本身的活动电位脉冲。 神经元的连接具有可塑性，表现在其活动电位脉冲的传递强度依靠神经传递化学物质的释放量及突触间隙的变化量，可以进行调节。 激活函数 通常有如下一些性质： 非线性： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候（即f(x)=x），就不满足这个性质了，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。 可微性： 当优化方法是基于梯度的时候，这个性质是必须的。 单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。 f(x)≈x： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。 输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate. 连续并可导（允许少数点上不可导）的非线性函数。 激活函数及其导函数要尽可能的简单 激活函数的导函数的值域要在一个合适的区间内 单调递增 激活函数 ​ 非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（bias shift），并进一步使得梯度下降的收敛速度变慢。 计算上更加高效 生物学合理性 单侧抑制、宽兴奋边界 在一定程度上缓解梯度消失问题 高斯误差线性单元（Gaussian Error Linear Unit，GELU） 其中P(X ≤ x)是高斯分布N(µ,σ 2 )的累积分布函数，其中µ,σ为超参数，一般设µ = 0,σ = 1即可 由于高斯分布的累积分布函数为S型函数，因此GELU可以用Tanh函数或Logistic函数来近似 常见激活函数及其导数表格 神经网络构建 M-P 神经网络（首个） 人工神经网络常见的连接形式有： 1、单神经元（M-P模型、单感知机、单连续感知机）； 2、单层前向连接的神经网络； 3、多层前向连接的神经网络（BP、RBF）； 4、单层带有反馈连接的神经网络（Hopfield网）； 5、多层回归（递归）神经网络（RNN）； 6、局部连接的人工神经网络（细胞神经网、小脑模型等）。 BP神经网络（优秀博客） 概述： 正向传播： ​ 输入样本－－－输入层－－－各隐层－－－输出层 判断是否转入反向传播阶段： ​ 若输出层的实际输出与期望的输出（教师信号）不符 误差反传 ​ 误差以某种形式在各层表示－－－－修正各层单元的权值 网络输出的误差减少到可接受的程度或者进行到预先设定的学习次数为止 改进的BP算法 BP算法的不足 梯度下降法的不足，是BP算法收敛速度慢的原因，有改进的BP算法克服其不足，如： 具有阻尼项的权值调整算法 变步长算法 消除样本输入顺序影响的改进算法 即使用一批学习样本产生的总误差来调整权值，用公式表示如下： $\\Delta w_{i j} = \\sum \\Delta_{\\mathrm{p}} w_{i j} = \\sum_{k = 1}^{m} \\delta_{o}(k) h o_{h}(k)$ 解决了因样本输入顺序引起的精度问题和训练的抖动问题。但是，该算法的收敛速度相对来说还是比较慢的。 附加动量的改进算法 在反向传播法的基础上在每一个权值（或阈值）的变化上加上一项正比于上一次权值（或阈值）变化量的值，并根据反向传播法来产生新的权值（或阈值）变化 $\\Delta w(k+1)=\\left(1-m_{c}\\right) \\eta \\nabla \\mathrm{f}(w(k))+m_{c}(w(k)-w(k-1))$ 可以防止的出现即最后一次权值的变化量为0，有助于使网络从误差曲面的局部极小值中跳出。但对于大多数实际应用问题，该法训练速度仍然很慢。 采用自适应调整参数的改进算法 采用自适应调整参数的改进算法的基本设想是学习率应根据误差变化而自适应调整，以使权系数调整向误差减小的方向变化，其迭代过程可表示为 ： $w(k+1)=w(k)-\\eta \\nabla f(w(k))$ 在很小的情况下，采用自适应调整参数的改进算法仍然存在权值的修正量很小的问题，致使学习率降低。 使用弹性方法的改进算法 BP网络通常采用Sigmoid隐含层。当输入的函数很大时，斜率接近于零，这将导致算法中的梯度幅值很小，可能使网络权值的修正过程几乎停顿下来。弹性方法只取偏导数的符号，而不考虑偏导数的幅值。其权值修正的迭代过程可表示为 ： $w(k+1)=w(k)-(w(k)-w(k-1)) \\operatorname{sign}(\\nabla f(w(k)))$ 在弹性BP算法中，当训练发生振荡时，权值的变化量将减小；当在几次迭代过程中权值均朝一个方向变化时，权值的变化量将增大。因此，使用弹性方法的改进算法，其收敛速度要比前几种方法快得多 使用拟牛顿法的改进算法 梯度法的缺点是搜索过程收敛速度较慢，牛顿法在搜索方向上比梯度法有改进，它不仅利用了准则函数在搜索点的梯度，而且还利用了它的二次导数，就是说利用了搜索点所能提供的更多信息，使搜索方向能更好地指向最优点。它的迭代方程为 ： $\\Delta w(k+1)=w(k)-D^{-1} \\nabla \\mathrm{f}(w(k))$ 收敛速度比一阶梯度快，但计算又较复杂，比较典型的有BFGS拟牛顿法和一步正切拟牛顿法。 基于共轭梯度法的改进算法 梯度下降法收敛速度较慢，而拟牛顿法计算又较复杂，共轭梯度法则力图避免两者的缺点。共轭梯度法也是一种改进搜索方向的方法，它是把前一点的梯度乘以适当的系数，加到该点的梯度上，得到新的搜索方向。其迭代方程为 ：$$\\begin{array}\\Delta w(k+1) &amp; = &amp; w(k)+\\rho(k) S(k) \\S(k) &amp; = &amp; -\\nabla \\mathrm{f}(w(k))+v(k-1) S(k-1) \\v(k-1) &amp; = &amp; \\frac{|\\nabla \\mathrm{f}(w(k))|^{2}}{|\\nabla \\mathrm{f}(w(k-1))|^{2}}\\end{array}$$ 共轭梯度法比大多数常规的梯度下降法收敛快，并且只需增加很少的存储量和计算量。 补充深层前馈神经网络 通用近似定理 根据通用近似定理，对于具有线性输出层和至少一个使用“挤压”性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够，它可以以任意的精度来近似任何从一个定义在实数空间中的有界闭集函数。 应用到机器学习神经网络可以作为一个“万能”函数来使用，可以用来进行复杂的特征转换，或逼近一个复杂的条件分布。 ​ 优化问题 难点 参数过多，影响训练 非凸优化问题：即存在局部最优而非全局最优解，影响迭代 梯度消失问题，下层参数比较难调 参数解释起来比较困难 需求 计算资源要大 数据要多 算法效率要好：即收敛快 未完待续……","link":"/2022/03/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"Python爬虫学习（二）——微博热搜爬取+词云","text":"微博热搜url: https://s.weibo.com/top/summary?cate=realtimehot 可以看到 该网页是服务器端渲染返回html 使用正则表达式进行匹配 12345678910111213141516171819202122import jiebafrom wordcloud import WordCloudimport pandas as pdfrom imageio import imreadimport matplotlib.pyplot as pltimport reimport requestsheaders = { &quot;cookie&quot;: &quot;&quot;, &quot;user-agent&quot;: &quot;&quot; #写自己的}url = 'https://s.weibo.com/top/summary?cate=realtimehot'resp = requests.get(url, headers=headers)page_content = resp.textresp.close()rootss = re.compile(r'&lt;tr class=&quot;&quot;&gt;.*?&lt;a href=&quot;(?P&lt;urls&gt;.*?)&quot;.*?&gt;(?P&lt;titles&gt;.*?)&lt;/a&gt;.*?&lt;span&gt;.*?(?P&lt;nums&gt;\\d+)&lt;/span&gt;', re.S)rootssss = rootss.findall(page_content)df = pd.DataFrame(rootssss)df.columns = ['url', 'title', 'hot'] 注意re.S，headers需要时常更换 使用jieba进行分词，注意停用词的设置，使用WordCloud进行词云制作，注意相关背景、字体设置api，如下 1234567891011121314151617181920212223all_words = cut_words(df.iloc[:, 1])stop = ['的', '你', '了', '将', '为', '例', ' ', '多', '再', '有', '是', '等', '天', '次', '称']words_cut = []for word in all_words: if word not in stop: words_cut.append(word)word_count = pd.Series(words_cut).value_counts()back_ground = imread(&quot;1.jpg&quot;)wc = WordCloud( font_path=&quot;C:\\\\Windows\\\\Fonts\\\\simhei.ttf&quot;, # 设置字体 background_color=&quot;white&quot;, # 设置词云背景颜色 max_words=1000, # 词云允许最大词汇数 mask=back_ground, # 词云形状 scale=20, max_font_size=200, # 最大字体大小 random_state=50 # 配色方案的种数)wc1 = wc.fit_words(word_count) # 生成词云plt.figure()plt.imshow(wc1)plt.axis(&quot;off&quot;)plt.show()wc.to_file(&quot;WordCloudNew.png&quot;) 结果如下","link":"/2022/03/16/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A01/"},{"title":"Python爬虫学习（一）","text":"爬虫主要是用程序模拟实际请求，因为学过前端技术，所以很多地方理解起来并不困难。 1 爬虫初体验12345678910from urllib.request import urlopenurl = &quot;http://www.baidu.com&quot;resp = urlopen(url)str1 = resp.read().decode(&quot;utf-8&quot;)print(str1)with open(&quot;mybaidu.html&quot;, mode=&quot;w&quot;, encoding=&quot;utf-8&quot;) as f: f.write(str1)print(&quot;over!&quot;) 这里属于最简单的爬虫，获取百度的源代码 2 Web请求的两种方式 服务端渲染 客户端渲染 get post…… 3 HTTP协议 请求头 响应头 4 request入门 011234567891011121314import requestsurl = 'https://cn.bing.com/search?q=%E5%91%A8%E6%9D%B0%E4%BC%A6'dic = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36 Edg/98.0.1108.56&quot;,}resp = requests.get(url, headers=dic)print(resp)print(resp.text)str1 = resp.textwith open(&quot;requestsrumen.html&quot;, mode=&quot;w&quot;, encoding=&quot;utf-8&quot;) as f: f.write(str1)resp.close() Get请求 获取必应网站搜索内容 体会 请求头 的重要作用，如果没有请求头，获取到的内容是什么样子呢 注意 打开文件的编码 resp.text resp.close() 5 request入门0212345678910111213import requestsurl = &quot;https://fanyi.baidu.com/sug&quot;s = input(&quot;a word&quot;)dat = { &quot;kw&quot;: s}resp = requests.post(url, data=dat)print(resp.json())resp.close() Post请求 关键在于使用F12 Network找到百度翻译的相关请求 请求字段写在后面data=dat 6 request入门031234567891011121314151617181920import requestsurl = &quot;https://movie.douban.com/j/chart/top_list&quot;param = { &quot;type&quot;: &quot;10&quot;, &quot;interval_id&quot;: &quot;100:90&quot;, &quot;action&quot;: &quot;&quot;, &quot;start&quot;: &quot;0&quot;, &quot;limit&quot;: &quot;20&quot;}headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36 Edg/98.0.1108.56&quot;}resp = requests.get(url=url, params=param, headers = headers)print(resp.json())resp.close() 豆瓣电影类型榜单爬取 ， get请求，注意 params + headers （这里有数目限制 如需全部须循环） 7 数据解析 re正则表达式 （在线测试正则表达式 https://tool.oschina.net/regex/） .除了换行符以外的任何字符 \\w 匹配字母或数字或下划线 \\s 匹配任意空白字符 \\d 匹配数字 \\n 匹配换行符 \\t 匹配一个制表符 ^ 匹配字符串的开始 $ 匹配字符串的结尾 \\W \\D \\S 非…. a|b 匹配字符a或字符b () 匹配括号内的表达式，也表示一个组 […] 匹配字符组中的字符 [^…] 匹配除了字符组中的字符的所有字符 重复0次或很多次 + 重复一次或更多次 ？重复零次或一次{n} 重复n次 {n,}重复n次或更多次 {n,m}重复n次到m次 .* 贪婪匹配.? 让尽可能少的匹配 惰性匹配 8 re模块匹配12345678910111213141516171819202122import relst = re.findall(r&quot;\\d+&quot;, &quot;我的电话号码是10086&quot;)print(lst)it = re.finditer(r&quot;\\d+&quot;, &quot;我的电话号码是10086&quot;)print(it)for i in it: print(i.group())s = re.search(r&quot;\\d+&quot;, &quot;我的电话号码是10086&quot;)print(s.group())#m = re.match() 从头开始匹配#预加载正则表达式obj = re.compile(r&quot;我的电话号码是(?P&lt;phonenum&gt;\\d+)&quot;)its = obj.finditer(&quot;我的电话号码是10086&quot;)print(its)for i in its: print(i.group()) print(i.group(&quot;phonenum&quot;)) 9 DoubanTop100这里是服务器直接渲染html传给前端的，需用正则表达式匹配 12345678910111213141516171819202122232425262728293031import requestsimport reimport csvheaders = { &quot;user-agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36 Edg/98.0.1108.56&quot;}page_content = &quot;&quot;for i in range(10): paras = { &quot;start&quot;: i*25 } url = &quot;https://movie.douban.com/top250&quot; resp = requests.get(url, params=paras, headers=headers) page_content += resp.text resp.close()obj = re.compile(r'&lt;li&gt;.*?&lt;div class=&quot;item&quot;&gt;.*?&lt;span class=&quot;title&quot;&gt;(?P&lt;title&gt;.*?)&lt;/span&gt;' r'.*?&lt;p class=&quot;&quot;&gt;.*?&lt;br&gt;(?P&lt;year&gt;.*?)&amp;nbsp', re.S)its = obj.finditer(page_content)f = open(&quot;datadouban250.csv&quot;, mode=&quot;w&quot;, encoding=&quot;utf-8&quot;, newline='')csvwriter = csv.writer(f)for i in its: dic = i.groupdict() dic['year'] = dic['year'].strip() csvwriter.writerow(dic.values()) # print(i.group(&quot;title&quot;)) # print(i.group(&quot;year&quot;).strip())f.close()print(&quot;Over!&quot;)","link":"/2022/03/16/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A02/"},{"title":"Snownlp训练微博语料并测试","text":"12import pandas as pdfrom snownlp import sentiment 12pos = pd.read_csv(&quot;train.txt&quot;, header=None, names=['id', 'pos', 'main'])pos .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id pos main 0 4231265311393455 1 #静下心来听音乐# “书中自有黄金屋，书中自有颜如玉”。沿着岁月的长河跋涉，或是风光... 1 4231265307198472 0 这是英超被黑的最惨的一次[二哈][二哈]十几年来，中国只有孙继海，董方卓，郑智，李铁登陆过英... 2 4231265303587877 1 【俞曾港：专业聚焦和产业链延伸是企业“走出去”根本要义】中国远洋海运集团副总经理俞曾港4月2... 3 4231265303003094 1 看《流星花园》其实也还好啦，现在的观念以及时尚眼光都不一样了，或许十几年之后的人看我们的现在... 4 4231265290813194 1 汉武帝的罪己诏的真实性尽管存在着争议，然而“轮台罪己诏”作为中国历史上第一份皇帝自我批评的文... ... ... ... ... 9995 4234961114403222 0 火车上碰见这种占别人位置还理直气壮的王八蛋真的心累，报了乘警半天不见人，只好祝病魔早日战胜之... 9996 4234960615356814 0 倒霉催的，坐上晚点一个多小时的汽车🚗，在高速上司机叔叔说他没听清我说话，so我一路超想上厕所... 9997 4234960451562301 0 急诊第一天上班，说不上的心累，这漫长的两个月如何过啊[悲伤][悲伤][悲伤] ​ 9998 4234960371996125 0 我每个月供着爱奇艺，网易云，快连，芒果TV，包图网，这些都是大企业啊，我也是心累😂😂😂😱😱😱 ​ 9999 4234960283775479 0 其实这种两家毫无交集 因为一个粉丝单方面撩骚就撕成一片上升成两家战火的情况真的很煤意思 要是... 10000 rows × 3 columns 12pos = pos.drop(['id'],axis=1)pos .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pos main 0 1 #静下心来听音乐# “书中自有黄金屋，书中自有颜如玉”。沿着岁月的长河跋涉，或是风光... 1 0 这是英超被黑的最惨的一次[二哈][二哈]十几年来，中国只有孙继海，董方卓，郑智，李铁登陆过英... 2 1 【俞曾港：专业聚焦和产业链延伸是企业“走出去”根本要义】中国远洋海运集团副总经理俞曾港4月2... 3 1 看《流星花园》其实也还好啦，现在的观念以及时尚眼光都不一样了，或许十几年之后的人看我们的现在... 4 1 汉武帝的罪己诏的真实性尽管存在着争议，然而“轮台罪己诏”作为中国历史上第一份皇帝自我批评的文... ... ... ... 9995 0 火车上碰见这种占别人位置还理直气壮的王八蛋真的心累，报了乘警半天不见人，只好祝病魔早日战胜之... 9996 0 倒霉催的，坐上晚点一个多小时的汽车🚗，在高速上司机叔叔说他没听清我说话，so我一路超想上厕所... 9997 0 急诊第一天上班，说不上的心累，这漫长的两个月如何过啊[悲伤][悲伤][悲伤] ​ 9998 0 我每个月供着爱奇艺，网易云，快连，芒果TV，包图网，这些都是大企业啊，我也是心累😂😂😂😱😱😱 ​ 9999 0 其实这种两家毫无交集 因为一个粉丝单方面撩骚就撕成一片上升成两家战火的情况真的很煤意思 要是... 10000 rows × 2 columns 12nav = pos[pos['pos'] == 0]nav .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pos main 1 0 这是英超被黑的最惨的一次[二哈][二哈]十几年来，中国只有孙继海，董方卓，郑智，李铁登陆过英... 5 0 我努力的等着我想要的结局。但在某个时刻，我感觉到似乎早已有了结局。不知是我没有感觉到，还是感... 15 0 “狗都不如的中国人”？？就你这样儿还配支教？！别给黑龙江人抹黑行吗？！ #日杂档案# ​ 17 0 自己选的路，咬着牙走吧，缘分这东西说不清道不明，不知道为什么喜欢了，也不知道为什么就这么久，... 20 0 【泰勒公寓再遭疯狂粉丝闯入 在其家中洗澡睡觉】据外媒，泰勒·斯威夫特位于纽约的公寓再被狂热歌... ... ... ... 9995 0 火车上碰见这种占别人位置还理直气壮的王八蛋真的心累，报了乘警半天不见人，只好祝病魔早日战胜之... 9996 0 倒霉催的，坐上晚点一个多小时的汽车🚗，在高速上司机叔叔说他没听清我说话，so我一路超想上厕所... 9997 0 急诊第一天上班，说不上的心累，这漫长的两个月如何过啊[悲伤][悲伤][悲伤] ​ 9998 0 我每个月供着爱奇艺，网易云，快连，芒果TV，包图网，这些都是大企业啊，我也是心累😂😂😂😱😱😱 ​ 9999 0 其实这种两家毫无交集 因为一个粉丝单方面撩骚就撕成一片上升成两家战火的情况真的很煤意思 要是... 4504 rows × 2 columns 12pos = pos[pos['pos'] == 1]pos .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pos main 0 1 #静下心来听音乐# “书中自有黄金屋，书中自有颜如玉”。沿着岁月的长河跋涉，或是风光... 2 1 【俞曾港：专业聚焦和产业链延伸是企业“走出去”根本要义】中国远洋海运集团副总经理俞曾港4月2... 3 1 看《流星花园》其实也还好啦，现在的观念以及时尚眼光都不一样了，或许十几年之后的人看我们的现在... 4 1 汉武帝的罪己诏的真实性尽管存在着争议，然而“轮台罪己诏”作为中国历史上第一份皇帝自我批评的文... 6 1 儿时的光阴永远是最美好的，从你还在襁褓中开始，一路陪伴我做琴童的日子，之后你也不可豁免滴成了... ... ... ... 9742 1 一起看了最恶心的电影，两天半吃啥都觉得不好吃，虽然吃了拾味馆，但味道和海南的还是有丝丝不一样... 9780 1 真他妈笑死了 鹦鹉＝恶心人哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈... 9955 1 刚看到一个营销博主说谢霆锋现在已经扑街又没有流量，只能靠微博上当段子手博取好感赚钱。哈哈哈哈... 9962 1 #滴滴司机打人# 心累，本想找到那个司机，让他接收法律的制裁就好了。看到了滴滴公关的... 9967 1 上班忙到现在 心累 五一快乐 Just Girly Things ​ 5496 rows × 2 columns 12nav['main'].to_csv(&quot;nav_train.txt&quot;, sep=',', encoding='utf-8', header=None, index=None)pos['main'].to_csv(&quot;pos_train.txt&quot;, sep=',', encoding='utf-8', header=None, index=None) 12sentiment.train('nav_train.txt', 'pos_train.txt')sentiment.save('weibo.marshal') 12strtemp = SnowNLP(u'我不知道明天会是什么样子，一点儿也不期盼，但我也不会逃避，我会成长并且自治愈，我超厉害的[加油][加油][并不简单][二哈]​ ​​​')print(strtemp.sentiments) 0.5407771941400631 12345678910from snownlp import SnowNLPdef getSen(str): s = SnowNLP(str) ss = s.sentiments c=0 if ss&gt;0.5: c=1 else: c=0 return c 12test = pd.read_csv(&quot;test.txt&quot;, header=None, names=['id', 'pos', 'main'], encoding='utf-8')test .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id pos main 0 4235713744988722 0 阿拉蕾摔倒了，穿的太厚了起不来，董力把她抱了起来！ #阿拉蕾# 围观明星大宝贝的秒... 1 4235713744437008 1 我不知道明天会是什么样子，一点儿也不期盼，但我也不会逃避，我会成长并且自治愈，我超厉害的[加... 2 4235713740835092 1 生日快乐送给纹上小猪佩奇的社会尘[小黄人剪刀手] ​ 3 4235713740787712 1 【27】速溶汤30包，包括：菠菜蛋花、西红柿蛋花、鲜蔬芙蓉汤、紫菜蛋花汤、小白菜蛋花汤共5种... 4 4235713740787185 0 上班不是因为工作累，更是心累[悲伤]哎～ 复杂的人心 简单点不好吗？ #hi五月# ... ... ... ... ... 495 4236310841130900 1 梦见自己在高考，然后在我作文还没写完的时候，突然就要收卷，但见我400个字，监考老师就如同我... 496 4236310837769167 0 太子最近的爱好是突然跳我身上来[允悲]刚要睡着，肚子上一沉，就看到这么大个脑袋[哼]打了下屁... 497 4236310833415948 1 下夜班快一点了到家，老爸蒸了馒头，包了饺子等我，珍惜这平凡的日子，感恩我如此美好的生活 ​ 498 4236310833415822 1 夏天有很多水果陪我 一点都不孤单[吃瓜] ​ 499 4236310833015868 0 他说我是一个很感性细腻的人只要熟一点都看得出来但希望往后 把情绪调回静音模式最近变得很浮躁 ... 500 rows × 3 columns 12test['score'] = test['main'].apply(getSen)test .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id pos main score 0 4235713744988722 0 阿拉蕾摔倒了，穿的太厚了起不来，董力把她抱了起来！ #阿拉蕾# 围观明星大宝贝的秒... 1 1 4235713744437008 1 我不知道明天会是什么样子，一点儿也不期盼，但我也不会逃避，我会成长并且自治愈，我超厉害的[加... 1 2 4235713740835092 1 生日快乐送给纹上小猪佩奇的社会尘[小黄人剪刀手] ​ 1 3 4235713740787712 1 【27】速溶汤30包，包括：菠菜蛋花、西红柿蛋花、鲜蔬芙蓉汤、紫菜蛋花汤、小白菜蛋花汤共5种... 1 4 4235713740787185 0 上班不是因为工作累，更是心累[悲伤]哎～ 复杂的人心 简单点不好吗？ #hi五月# ... 1 ... ... ... ... ... 495 4236310841130900 1 梦见自己在高考，然后在我作文还没写完的时候，突然就要收卷，但见我400个字，监考老师就如同我... 0 496 4236310837769167 0 太子最近的爱好是突然跳我身上来[允悲]刚要睡着，肚子上一沉，就看到这么大个脑袋[哼]打了下屁... 0 497 4236310833415948 1 下夜班快一点了到家，老爸蒸了馒头，包了饺子等我，珍惜这平凡的日子，感恩我如此美好的生活 ​ 0 498 4236310833415822 1 夏天有很多水果陪我 一点都不孤单[吃瓜] ​ 1 499 4236310833015868 0 他说我是一个很感性细腻的人只要熟一点都看得出来但希望往后 把情绪调回静音模式最近变得很浮躁 ... 0 500 rows × 4 columns 1print((test[&quot;pos&quot;] == test[&quot;score&quot;]).sum()/500) 0.834","link":"/2022/03/18/Snownlp-weibo/"}],"tags":[{"name":"感想","slug":"感想","link":"/tags/%E6%84%9F%E6%83%B3/"},{"name":"记录","slug":"记录","link":"/tags/%E8%AE%B0%E5%BD%95/"},{"name":"《深度学习》","slug":"《深度学习》","link":"/tags/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B/"},{"name":"线性代数","slug":"线性代数","link":"/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"引言","slug":"引言","link":"/tags/%E5%BC%95%E8%A8%80/"},{"name":"《神经网络与深度学习》","slug":"《神经网络与深度学习》","link":"/tags/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B/"},{"name":"前馈神经网络","slug":"前馈神经网络","link":"/tags/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"爬虫","slug":"爬虫","link":"/tags/%E7%88%AC%E8%99%AB/"},{"name":"学习笔记","slug":"学习笔记","link":"/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"代码笔记","slug":"代码笔记","link":"/tags/%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/"}],"categories":[{"name":"日常","slug":"日常","link":"/categories/%E6%97%A5%E5%B8%B8/"},{"name":"读书笔记","slug":"读书笔记","link":"/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"感想","slug":"日常/感想","link":"/categories/%E6%97%A5%E5%B8%B8/%E6%84%9F%E6%83%B3/"},{"name":"《深度学习》","slug":"读书笔记/《深度学习》","link":"/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B/"},{"name":"《神经网络与深度学习》","slug":"读书笔记/《神经网络与深度学习》","link":"/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B/"},{"name":"学习笔记","slug":"学习笔记","link":"/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"爬虫","slug":"学习笔记/爬虫","link":"/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%88%AC%E8%99%AB/"},{"name":"代码笔记","slug":"代码笔记","link":"/categories/%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/"},{"name":"nlp","slug":"代码笔记/nlp","link":"/categories/%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/nlp/"}]}